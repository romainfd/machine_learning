{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Convolutional Neural Networks for Text Categorization\n",
    "\n",
    "\n",
    "In this lab, we will implement a convolutional neural network (CNN) to perform binary movie review classification (positive/negative) using the [Keras](https://keras.io/getting-started/faq/}) library. \n",
    "\n",
    "You will recieve an overview of using Keras. It provides an API over TensorFlow, which you worked with in the last lab.\n",
    "\n",
    "The architecture of the CNN we will develop is described in [CNNs for Sentence Classification (Kim, EMNLP'14)](https://arxiv.org/abs/1408.5882). We will also visualize document embeddings and predictive regions in the input documents, following [Effective Use of Word Order for Text Categorization with Convolutional Neural Networks (Johnson and Zhang, NAACL'15)](https://arxiv.org/pdf/1412.1058.pdf), and first-order derivate saliency maps, following [Visualizing and Understanding Neural Models in NLP (Li et al., NAACL'16)](https://arxiv.org/abs/1506.01066). \n",
    "\n",
    "\n",
    "\n",
    "### Reading data and preprocessing\n",
    "For our experiments, we will use the [sentence polarity dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz). The dataset was collected by Pang and Lee and consists of 5,331 positive and 5,331 negative snippets acquired from Rotten Tomatoes. Snippets were automatically labeled using the labels provided by Rotten Tomatoes. The positive and negative reviews are stored into the `rt-polarity.pos` and `rt-polarity.neg` files, respectively. Let's first read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1a532e47565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def load_documents(filename):\n",
    "    docs =[]\n",
    "\n",
    "    with open(filename, encoding='utf8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            docs.append(line[:-1])\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = list()\n",
    "labels = list()\n",
    "\n",
    "docs_pos = load_documents('data/rt-polarity.pos')\n",
    "docs.extend(docs_pos)\n",
    "labels.extend([1]*len(docs_pos))\n",
    "\n",
    "docs_neg = load_documents('data/rt-polarity.neg')\n",
    "docs.extend(docs_neg)\n",
    "labels.extend([0]*len(docs_neg))\n",
    "\n",
    "y = np_utils.to_categorical(labels)\n",
    "\n",
    "print(\"A positive review:\", docs_pos[0])\n",
    "print('\\n')\n",
    "print(\"A negative review:\", docs_neg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents that are contained in the dataset have already undergone some preprocessing. Therefore, we will only remove some punctuation marks, diacritics, and non letters, if any. Furthermore, we will represent each document as a list of tokens. Use the ``preprocessing`` function (already implemented) to preprocess the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().split()\n",
    "\n",
    "    \n",
    "def preprocessing(docs):\n",
    "    preprocessed_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        preprocessed_docs.append(clean_str(doc))\n",
    "\n",
    "    return preprocessed_docs\n",
    "\n",
    "\n",
    "#your code here\n",
    "docs = map(preprocessing, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we will extract the vocabulary of the dataset. We will store the vocabulary in a dictionary where keys are terms and values correspond to indices. Hence, each term will be assigned a unique index. The minimum index will be equal to 1, while the maximum index will be equal to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(processed_docs):\n",
    "    vocab = dict()\n",
    "\n",
    "    for doc in processed_docs:\n",
    "        for word in doc:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(processed_docs)\n",
    "print(\"Size of the vocabulary:\", len(vocab))\n",
    "print(\"Index of term 'bad':\", vocab[\"bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a set of 300-dimensional word embeddings learned with word2vec on the GoogleNews dataset. The embeddings can be downloaded from https://code.google.com/archive/p/word2vec/Google, under the section \"Pre-trained word and phrase vectors\". Using `gensim`, we can extract only the vectors of the words found in our vocabulary. Terms not present in the set of pre-trained words are initialized randomly (uniformly in [âˆ’0.25, 0.25]). Before executing the code, set the path for the file that contains the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def load_embeddings(fname, vocab):\n",
    "    embeddings = np.zeros((len(vocab)+1, 300))\n",
    "    \n",
    "    model = KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "    for word in vocab:\n",
    "        if word in model:\n",
    "            embeddings[vocab[word]] = model[word]\n",
    "        else:\n",
    "            embeddings[vocab[word]] = np.random.uniform(-0.25, 0.25, 300)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "path_to_embeddings = '...' # fill in the path\n",
    "embeddings = load_embeddings(path_to_embeddings, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the size of the largest document and create a matrix whose rows correspond to documents. Each row contains the indices of the terms appearing in the document and preserves the order of the terms in the document. That is, the first component of a row contains the index of the first term of the corresponding document, the second component contains the index of the second term etc. Documents whose length is shorter than that of the longest document are padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then use the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function of scikit-learn to split our dataset randomly into a training and a test set. Set the size of the test set to 0.1 and the random_state to 91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the CNN\n",
    "For efficiency reasons, we will only implement two branches of the following architecture: \n",
    "\n",
    "<img src=\"https://github.com/Tixierae/deep_learning_NLP/raw/master/cnn_illustration.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "A branch is the part of the architecture that corresponds to a given filter size (e.g., the upper red part is one branch).\n",
    "\n",
    "\n",
    "Let's first set the values of our parameters. Below are some explanations about the role of some parameters. The others are pretty self-explanatory.\n",
    "* `epochs`: increasing the number of epochs may lead to overfitting when max_size is small (especially since dataset is small in the first place)\n",
    "* `my_optimizer`: `adam` proved better than `SGD` and `Adadelta` in preliminary experiments\n",
    "* `my_patience`: for early stopping strategy (number of epochs without improvement to wait before we stop training)\n",
    "* `do_static`: determines if the embeddings will be fine-tuned during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_save = 'cnn_text_categorization.hdf5'\n",
    "print('best model will be saved with name:',name_save)\n",
    "\n",
    "word_vector_dim = int(3e2)\n",
    "do_static = True\n",
    "nb_filters = 100\n",
    "filter_size_a = 3\n",
    "filter_size_b = 4\n",
    "drop_rate = 0.5\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "my_optimizer = 'adam' \n",
    "my_patience = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have only made use of the Sequential model. The Sequential model offers limited flexibility and may not be suitable for neural networks with multiple inputs and outputs. On the other hand, the [functional API](https://keras.io/getting-started/functional-api-guide/) makes it easy to manipulate a large number of intertwined datastreams. Our CNN consists of two branches whose outputs are concatenated to produce a single vector representation for each document (i.e., the multicolor vector shown in the figure above). We will use the functional API to implement the CNN. Unlike the Sequential model, in the case of the functional API it is necessary to create and define a standalone Input layer that specifies the shape of input data. The input layer takes as input a tuple that indicates the dimensionality of the input data. When the input data is one-dimensional (as in our case), the shape must explicitly leave room for the shape of the mini-batch size. Therefore, the shape tuple is always defined with a hanging last dimension when the input is one-dimensional:\n",
    "```\n",
    "my_input = Input(shape=(dimension,))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers in the model are connected pairwise. Hence, each layer takes as input either the input data (e.g., the input layer) or the output of another layer. We will first define and [Embedding layer](https://keras.io/layers/embeddings/). The Embedding layer requires the input data to be integer encoded, so that each word is represented by a unique integer. The Embedding layer can be initialized either with random weights and learn an embedding for all of the words in the training set or with pre-trained word embeddings. In our case, it will be initialized with the 300-dimensional word embeddings that we have already loaded. The Embedding layer must specify 3 arguments: (1) `input_dim`: the size of the vocabulary, (2) `output_dim`: the size of the vector space in which the words have been embedded (i.e., 300 in our case), and (3) `input_length`: the maximum length of the input documents. In case we initialize the layer with pre-trained embeddings, we must provide another argument (`weights`) which is list that contains a matrix whose i-th row contain the embedding of term with index i. For example, below we define an Embedding layer with a vocabulary of 100, embedding dimensionality equal to 64, maximum length of the input documents equal to 50, and the embeddings are contained in the matrix embed_matrix.\n",
    "```\n",
    "embedding = Embedding(input_dim=100,\n",
    "                      output_dim=64,\n",
    "                      weights=[embed_matrix],\n",
    "                      input_length=50\n",
    "                      ) (my_input)\n",
    "```\n",
    "\n",
    "Note that we also specify where the input comes from when defining a layer. Implement the embedding layer of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create the two branches of the CNN. Each branch takes as input the output of the Embedding layer and applies a [one-dimensional convolution layer](https://keras.io/layers/convolutional/#conv1d) followed by a [one-dimensional max-pooling operation](https://keras.io/layers/pooling/#maxpooling1d). \n",
    "\n",
    "The one-dimensional convolution layer must specify 3 arguments: (1) `filters`: the number of filters, (2) `kernel_size`: the length of the one-dimensional convolution window, and (3) `activation`: the activation function to use. For example, below we define an one-dimensional convolution layer with 100 filters, a convolution window of size 4, and a ReLU activation function.\n",
    "```\n",
    "conv = Conv1D(filters = 100,\n",
    "              kernel_size = 4,\n",
    "              activation = 'relu',\n",
    "              )(embedding)\n",
    "```\n",
    "\n",
    "The one-dimensional max-pooling operation just takes as input the output of the convolution layer. Implement the two branches of the CNN. For each branch, implement a one-dimensional convolution layer and a one-dimensional max-pooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next introduce a layer that [concatenates](https://keras.io/layers/merge/#concatenate_1) the outputs of the two branches. This layer takes as input the two vectors that were produced from the two branches and returns a single vector, the concatenation of the two inputs. We then add dropout and finally a fully-connected neural network with 2 neurons that will serve as out ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Dense, Concatenate\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating all of the layers and connecting them together, we can define the model. Keras provides a [Model class](https://keras.io/models/model/) that we can use to create a model from your created layers. It requires that you only specify the input and output layers. For example, given the an input and an output, we can define the model as follows:\n",
    "```\n",
    "model = Model(input, output)\n",
    "```\n",
    "After creating the model, you can compile it and set the loss function and optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally print the details of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of document embeddings before training\n",
    "We extract the output of the final embedding layer (before the softmax), which gives the encoding of the input document for some documents (`n_plot`) of the test set. We then visualize a low-dimensional map of the embeddings. We can see that before training, the documents are dispersed randomly in the space (which makes sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from keras import backend as K\n",
    "\n",
    "# in test mode, we should set the 'learning_phase' flag to 0 (we don't want to use dropout)\n",
    "get_doc_embedding = K.function([model.layers[0].input,K.learning_phase()],\n",
    "                               [model.layers[6].output])\n",
    "\n",
    "n_plot = 1000\n",
    "print('plotting embeddings of first',n_plot,'documents')\n",
    "\n",
    "doc_emb = get_doc_embedding([np.array(X_test[:n_plot]),0])[0]\n",
    "\n",
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2,perplexity=10) #https://lvdmaaten.github.io/tsne/\n",
    "doc_emb_pca = my_pca.fit_transform(doc_emb) \n",
    "doc_emb_tsne = my_tsne.fit_transform(doc_emb_pca)\n",
    "\n",
    "labels_plt = y_test[:n_plot,0].astype(np.int32)\n",
    "my_colors = ['blue','red']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for label in list(set(labels_plt)):\n",
    "    idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n",
    "    ax.scatter(doc_emb_tsne[idxs,0], \n",
    "               doc_emb_tsne[idxs,1], \n",
    "               c = my_colors[label],\n",
    "               label=str(label),\n",
    "               alpha=0.7,\n",
    "               s=10)\n",
    "\n",
    "ax.legend(scatterpoints=1)\n",
    "fig.suptitle('t-SNE visualization of CNN-based doc embeddings \\n (first 1000 docs from test set)',fontsize=10)\n",
    "fig.set_size_inches(6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN\n",
    "We train the model on CPU. Note you can get a significant speedup by using a GPU. We also add two callbacks:\n",
    "* the first one ensures that training stops after `my_patience` epochs without improvement in test set accuracy (early stopping strategy)\n",
    "* the second one (checkpointer) saves the model to disk for every epoch for which there is improvement. Therefore, at the end of training, the model saved on disk will be the one corresponding to the best epoch and we can reload it.\n",
    "\n",
    "Use the [fit](https://keras.io/models/model/#methods) function of Keras to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', # go through epochs as long as accuracy on validation set increases\n",
    "                               patience=my_patience,\n",
    "                               mode='max')\n",
    "\n",
    "# make sure that the model corresponding to the best epoch is saved\n",
    "checkpointer = ModelCheckpoint(filepath=name_save,\n",
    "                               monitor='val_acc',\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure we load the model corresponding to the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(name_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of document embeddings after training\n",
    "We can see that after only a few epochs, our model has already learned meaningful internal representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('plotting embeddings of first',n_plot,'documents')\n",
    "\n",
    "doc_emb = get_doc_embedding([np.array(X_test[:n_plot]),0])[0]\n",
    "\n",
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2,perplexity=10)\n",
    "doc_emb_pca = my_pca.fit_transform(doc_emb) \n",
    "doc_emb_tsne = my_tsne.fit_transform(doc_emb_pca)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for label in list(set(labels_plt)):\n",
    "    idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n",
    "    ax.scatter(doc_emb_tsne[idxs,0], \n",
    "               doc_emb_tsne[idxs,1], \n",
    "               c = my_colors[label],\n",
    "               label=str(label),\n",
    "               alpha=0.7,\n",
    "               s=10)\n",
    "\n",
    "ax.legend(scatterpoints=1)\n",
    "fig.suptitle('t-SNE visualization of CNN-based doc embeddings \\n (first 1000 docs from test set)',fontsize=10)\n",
    "fig.set_size_inches(6,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive text regions\n",
    "Here we follow the approach of [Effective Use of Word Order for Text Categorization with Convolutional Neural Networks (Johnson and Zhang, NAACL'15)](https://arxiv.org/pdf/1412.1058.pdf) (see Tables 5 and 6).\n",
    "\n",
    "The feature maps that we find at the output of the convolutional layer provide region embeddings (in an `nb_filters`-dimensional space). For a given branch associated with `filter_size`, there are `max_size-filter_size+1` regions of size `filter_size` for an input of size `max_size`. For a given document, we want to identify the `n_show` regions of each branch that are associated with the highest weights in the corresponding feature maps. \n",
    "\n",
    "We can see that to classify the test set documents (which the model has never seen), the CNN uses regions of the input documents that make sense to us as humans. It picks up the compliments (`\"glaring and unforgettable\"`, `\"a good yarn spinner\"`) and critics (`\"not merely unwatchable\"`, `\"but feeling pandered\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_regions(tokens, filter_size):\n",
    "    regions = []\n",
    "    regions.append(' '.join(tokens[:filter_size]))\n",
    "    for i in range(filter_size, len(tokens)):\n",
    "        regions.append(' '.join(tokens[(i-filter_size+1):(i+1)]))\n",
    "    return regions\n",
    "\n",
    "index_to_word = dict()\n",
    "for word in vocab:\n",
    "    index_to_word[vocab[word]] = word\n",
    "\n",
    "get_region_embedding_a = K.function([model.layers[0].input,K.learning_phase()],\n",
    "                                    [model.layers[2].output])\n",
    "\n",
    "get_region_embedding_b = K.function([model.layers[0].input,K.learning_phase()],\n",
    "                                    [model.layers[3].output])\n",
    "\n",
    "get_softmax = K.function([model.layers[0].input,K.learning_phase()],\n",
    "                         [model.layers[6].output])\n",
    "\n",
    "n_doc_per_label = 2\n",
    "idx_pos = [idx for idx in range(y_test.shape[0]) if y_test[idx,0]==1]\n",
    "idx_neg = [idx for idx in range(y_test.shape[0]) if y_test[idx,0]==0]\n",
    "my_idxs = idx_pos[:n_doc_per_label] + idx_neg[:n_doc_per_label]\n",
    "\n",
    "X_test_my_idxs = np.array([X_test[elt] for elt in my_idxs])\n",
    "y_test_my_idxs = [y_test[elt] for elt in my_idxs]\n",
    "\n",
    "reg_emb_a = get_region_embedding_a([X_test_my_idxs,0])[0]\n",
    "reg_emb_b = get_region_embedding_b([X_test_my_idxs,0])[0]\n",
    "\n",
    "# predictions are probabilities of belonging to class 1\n",
    "predictions = get_softmax([X_test_my_idxs,0])[0] \n",
    "# note: you can also use directly: predictions = model.predict(x_test[:100]).tolist()\n",
    "\n",
    "n_show = 3 # number of most predictive regions we want to display\n",
    "\n",
    "for idx,doc in enumerate(X_test_my_idxs):\n",
    "        \n",
    "    tokens = [index_to_word[elt] for elt in doc if elt!=0] # the 0 index is for padding\n",
    "    \n",
    "    # extract regions (sliding window over text)\n",
    "    regions_a = extract_regions(tokens, filter_size_a)\n",
    "    regions_b = extract_regions(tokens, filter_size_b)\n",
    "    \n",
    "    print('\\n *********')\n",
    "    print('===== text: =====')\n",
    "    print(' '.join(tokens))\n",
    "    print('===== label:',y_test_my_idxs[idx],'=====')\n",
    "    print('===== prediction:',predictions[idx],'=====')\n",
    "    norms_a = np.linalg.norm(reg_emb_a[idx,:,:],axis=1)\n",
    "    norms_b = np.linalg.norm(reg_emb_b[idx,:,:],axis=1)\n",
    "    print('===== most predictive regions of size',filter_size_a,': =====')\n",
    "    print([elt for idxx,elt in enumerate(regions_a) if idxx in np.argsort(norms_a)[-n_show:]]) # 'np.argsort' sorts by increasing order\n",
    "    print('===== most predictive regions of size',filter_size_b,': =====')\n",
    "    print([elt for idxx,elt in enumerate(regions_b) if idxx in np.argsort(norms_b)[-n_show:]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency maps\n",
    "Here we follow one of the approaches proposed in [Visualizing and Understanding Neural Models in NLP (Li et al., NAACL'16)](https://arxiv.org/abs/1506.01066).\n",
    "\n",
    "The idea is to rank the elements of the input document based on their influence on the prediction. An approximation can be given by the magnitudes of the first-order partial derivatives of the output of the model with respect to each word in the input document. The interpretation is that we identify which words in the document need to be *changed the least to change the class score the most*. The derivatives can be obtained by performing a single back-propagation pass. Note that here, we backpropagate the class score and not the loss (like we do during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tensors = [model.input, K.learning_phase()]\n",
    "saliency_input = model.layers[1].output # before split into branches\n",
    "saliency_output = model.layers[6].output # class score\n",
    "gradients = model.optimizer.get_gradients(saliency_output,saliency_input)\n",
    "compute_gradients = K.function(inputs=input_tensors,outputs=gradients)\n",
    "\n",
    "for idx,doc in enumerate(X_test_my_idxs):\n",
    "    matrix = compute_gradients([np.array([doc]),0])[0][0,:,:]\n",
    "    tokens = [index_to_word[elt] for elt in doc if elt!=0]\n",
    "    to_plot = np.absolute(matrix[:len(tokens),:])\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(to_plot, cmap=plt.cm.Blues, interpolation='nearest',aspect='auto')\n",
    "    ax.set_yticks(np.arange(len(tokens)))\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=32*10/len(tokens))\n",
    "    fig.colorbar(heatmap)\n",
    "    fig.set_size_inches(14,9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
